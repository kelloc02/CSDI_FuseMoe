{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_path = \"E:/Timeseries/dataset/physionet.org/files/mimiciv/1.0\"\n",
    "mm_dir = \"E:/Timeseries/dataset/multimodal\"\n",
    "\n",
    "output_dir = os.path.join(mm_dir, \"preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_notes = True\n",
    "include_cxr = True\n",
    "standard_scale = True\n",
    "include_missing = True\n",
    "include_ecg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/Timeseries/dataset/multimodal\\\\preprocessing\\\\ts_labs_vitals_icu.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ireg_vitals_ts_df = pd.read_pickle(os.path.join(output_dir, \"ts_vitals_icu.pkl\"))\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# imputed_vitals = pd.read_pickle(os.path.join(output_dir, \"imputed_ts_vitals_icu.pkl\"))\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m ireg_vitals_ts_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mts_labs_vitals_icu.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m imputed_vitals \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputed_ts_labs_vitals_icu.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m ireg_vitals_ts_df \u001b[38;5;241m=\u001b[39m ireg_vitals_ts_df[ireg_vitals_ts_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimedelta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\Programfiles\\miniconda3\\envs\\MulEHR\\lib\\site-packages\\pandas\\io\\pickle.py:187\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    186\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Programfiles\\miniconda3\\envs\\MulEHR\\lib\\site-packages\\pandas\\io\\common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/Timeseries/dataset/multimodal\\\\preprocessing\\\\ts_labs_vitals_icu.pkl'"
     ]
    }
   ],
   "source": [
    "# ireg_vitals_ts_df = pd.read_pickle(os.path.join(output_dir, \"ts_vitals_icu.pkl\"))\n",
    "# imputed_vitals = pd.read_pickle(os.path.join(output_dir, \"imputed_ts_vitals_icu.pkl\"))\n",
    "\n",
    "ireg_vitals_ts_df = pd.read_pickle(os.path.join(output_dir, \"ts_labs_vitals_icu.pkl\"))\n",
    "imputed_vitals = pd.read_pickle(os.path.join(output_dir, \"imputed_ts_labs_vitals_icu.pkl\"))\n",
    "\n",
    "ireg_vitals_ts_df = ireg_vitals_ts_df[ireg_vitals_ts_df['timedelta'] >= 0]\n",
    "imputed_vitals = imputed_vitals[imputed_vitals['timedelta'] >= 0]\n",
    "\n",
    "ireg_vitals_ts_df = ireg_vitals_ts_df[ireg_vitals_ts_df['timedelta'] <= 48]\n",
    "imputed_vitals = imputed_vitals[imputed_vitals['timedelta'] <= 48]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if include_notes:\n",
    "    notes_df = pd.read_pickle(os.path.join(output_dir, \"icu_notes_text_embeddings.pkl\"))\n",
    "    # notes_df = pd.read_pickle(os.path.join(output_dir, \"notes_text.pkl\"))\n",
    "    notes_df = notes_df[notes_df['stay_id'].notnull()]\n",
    "\n",
    "    notes_df = notes_df[notes_df['icu_time_delta'] >= 0]\n",
    "    notes_df = notes_df[notes_df['icu_time_delta'] <= 48]\n",
    "\n",
    "if include_cxr:\n",
    "    cxr_df = pd.read_pickle(os.path.join(output_dir, \"cxr_embeddings_icu.pkl\"))\n",
    "    cxr_df = cxr_df[cxr_df['icu_time_delta'] >= 0]\n",
    "    cxr_df = cxr_df[cxr_df['icu_time_delta'] <= 48]\n",
    "\n",
    "if include_ecg:\n",
    "    ecg_df = pd.read_pickle(os.path.join(output_dir, \"ecg_embeddings_icu.pkl\"))\n",
    "    ecg_df = ecg_df[ecg_df['icu_time_delta'] >= 0]\n",
    "    ecg_df = ecg_df[ecg_df['icu_time_delta'] <= 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>first_careunit</th>\n",
       "      <th>last_careunit</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14435996</td>\n",
       "      <td>28960964</td>\n",
       "      <td>31983544</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>2150-06-19 17:57:00</td>\n",
       "      <td>2150-06-22 18:33:54</td>\n",
       "      <td>3.025625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17609946</td>\n",
       "      <td>27385897</td>\n",
       "      <td>33183475</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>2138-02-05 18:54:00</td>\n",
       "      <td>2138-02-15 12:42:05</td>\n",
       "      <td>9.741725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15914763</td>\n",
       "      <td>28906020</td>\n",
       "      <td>36909804</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>2176-12-14 12:00:00</td>\n",
       "      <td>2176-12-17 11:47:01</td>\n",
       "      <td>2.990984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18599212</td>\n",
       "      <td>28538226</td>\n",
       "      <td>33267162</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>2129-06-01 16:27:39</td>\n",
       "      <td>2129-06-06 17:01:33</td>\n",
       "      <td>5.023542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14609218</td>\n",
       "      <td>20606189</td>\n",
       "      <td>34947848</td>\n",
       "      <td>Neuro Stepdown</td>\n",
       "      <td>Neuro Stepdown</td>\n",
       "      <td>2174-06-28 21:13:00</td>\n",
       "      <td>2174-07-05 17:01:32</td>\n",
       "      <td>6.825370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76528</th>\n",
       "      <td>12762280</td>\n",
       "      <td>23194856</td>\n",
       "      <td>39987031</td>\n",
       "      <td>Medical/Surgical Intensive Care Unit (MICU/SICU)</td>\n",
       "      <td>Medical/Surgical Intensive Care Unit (MICU/SICU)</td>\n",
       "      <td>2177-12-13 22:55:18</td>\n",
       "      <td>2177-12-15 23:25:26</td>\n",
       "      <td>2.020926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76529</th>\n",
       "      <td>11414090</td>\n",
       "      <td>25441049</td>\n",
       "      <td>39987151</td>\n",
       "      <td>Coronary Care Unit (CCU)</td>\n",
       "      <td>Coronary Care Unit (CCU)</td>\n",
       "      <td>2170-05-05 18:26:37</td>\n",
       "      <td>2170-05-11 13:54:30</td>\n",
       "      <td>5.811030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76533</th>\n",
       "      <td>16926631</td>\n",
       "      <td>25623508</td>\n",
       "      <td>39989105</td>\n",
       "      <td>Medical Intensive Care Unit (MICU)</td>\n",
       "      <td>Neuro Surgical Intensive Care Unit (Neuro SICU)</td>\n",
       "      <td>2197-02-24 02:03:46</td>\n",
       "      <td>2197-02-26 16:22:18</td>\n",
       "      <td>2.596204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76538</th>\n",
       "      <td>17577670</td>\n",
       "      <td>24221219</td>\n",
       "      <td>39993265</td>\n",
       "      <td>Medical/Surgical Intensive Care Unit (MICU/SICU)</td>\n",
       "      <td>Medical/Surgical Intensive Care Unit (MICU/SICU)</td>\n",
       "      <td>2154-01-03 22:50:26</td>\n",
       "      <td>2154-01-06 12:44:43</td>\n",
       "      <td>2.579363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76539</th>\n",
       "      <td>17840864</td>\n",
       "      <td>22695803</td>\n",
       "      <td>39999810</td>\n",
       "      <td>Neuro Intermediate</td>\n",
       "      <td>Neuro Intermediate</td>\n",
       "      <td>2115-12-01 00:37:00</td>\n",
       "      <td>2115-12-05 18:27:57</td>\n",
       "      <td>4.743715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36858 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id   stay_id  \\\n",
       "1        14435996  28960964  31983544   \n",
       "2        17609946  27385897  33183475   \n",
       "7        15914763  28906020  36909804   \n",
       "14       18599212  28538226  33267162   \n",
       "15       14609218  20606189  34947848   \n",
       "...           ...       ...       ...   \n",
       "76528    12762280  23194856  39987031   \n",
       "76529    11414090  25441049  39987151   \n",
       "76533    16926631  25623508  39989105   \n",
       "76538    17577670  24221219  39993265   \n",
       "76539    17840864  22695803  39999810   \n",
       "\n",
       "                                         first_careunit  \\\n",
       "1                                   Trauma SICU (TSICU)   \n",
       "2                                   Trauma SICU (TSICU)   \n",
       "7                                   Trauma SICU (TSICU)   \n",
       "14                                  Trauma SICU (TSICU)   \n",
       "15                                       Neuro Stepdown   \n",
       "...                                                 ...   \n",
       "76528  Medical/Surgical Intensive Care Unit (MICU/SICU)   \n",
       "76529                          Coronary Care Unit (CCU)   \n",
       "76533                Medical Intensive Care Unit (MICU)   \n",
       "76538  Medical/Surgical Intensive Care Unit (MICU/SICU)   \n",
       "76539                                Neuro Intermediate   \n",
       "\n",
       "                                          last_careunit              intime  \\\n",
       "1                                   Trauma SICU (TSICU) 2150-06-19 17:57:00   \n",
       "2                                   Trauma SICU (TSICU) 2138-02-05 18:54:00   \n",
       "7                                   Trauma SICU (TSICU) 2176-12-14 12:00:00   \n",
       "14                                  Trauma SICU (TSICU) 2129-06-01 16:27:39   \n",
       "15                                       Neuro Stepdown 2174-06-28 21:13:00   \n",
       "...                                                 ...                 ...   \n",
       "76528  Medical/Surgical Intensive Care Unit (MICU/SICU) 2177-12-13 22:55:18   \n",
       "76529                          Coronary Care Unit (CCU) 2170-05-05 18:26:37   \n",
       "76533   Neuro Surgical Intensive Care Unit (Neuro SICU) 2197-02-24 02:03:46   \n",
       "76538  Medical/Surgical Intensive Care Unit (MICU/SICU) 2154-01-03 22:50:26   \n",
       "76539                                Neuro Intermediate 2115-12-01 00:37:00   \n",
       "\n",
       "                  outtime       los  \n",
       "1     2150-06-22 18:33:54  3.025625  \n",
       "2     2138-02-15 12:42:05  9.741725  \n",
       "7     2176-12-17 11:47:01  2.990984  \n",
       "14    2129-06-06 17:01:33  5.023542  \n",
       "15    2174-07-05 17:01:32  6.825370  \n",
       "...                   ...       ...  \n",
       "76528 2177-12-15 23:25:26  2.020926  \n",
       "76529 2170-05-11 13:54:30  5.811030  \n",
       "76533 2197-02-26 16:22:18  2.596204  \n",
       "76538 2154-01-06 12:44:43  2.579363  \n",
       "76539 2115-12-05 18:27:57  4.743715  \n",
       "\n",
       "[36858 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icustays_df = pd.read_csv(os.path.join(mimic_iv_path, \"icu\", \"icustays.csv.gz\"), low_memory=False)\n",
    "icustays_df['intime'] = pd.to_datetime(icustays_df['intime'])\n",
    "icustays_df['outtime'] = pd.to_datetime(icustays_df['outtime'])\n",
    "\n",
    "icustays_df = icustays_df[icustays_df['los'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ireg_vitals_ts_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m valid_stay_ids \u001b[38;5;241m=\u001b[39m icustays_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m----> 3\u001b[0m ireg_vitals_ts_df \u001b[38;5;241m=\u001b[39m \u001b[43mireg_vitals_ts_df\u001b[49m[ireg_vitals_ts_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(valid_stay_ids)]\n\u001b[0;32m      4\u001b[0m imputed_vitals \u001b[38;5;241m=\u001b[39m imputed_vitals[imputed_vitals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(valid_stay_ids)]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_notes:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ireg_vitals_ts_df' is not defined"
     ]
    }
   ],
   "source": [
    "valid_stay_ids = icustays_df['stay_id'].unique()\n",
    "\n",
    "ireg_vitals_ts_df = ireg_vitals_ts_df[ireg_vitals_ts_df['stay_id'].isin(valid_stay_ids)]\n",
    "imputed_vitals = imputed_vitals[imputed_vitals['stay_id'].isin(valid_stay_ids)]\n",
    "\n",
    "if include_notes:\n",
    "    notes_df = notes_df[notes_df['stay_id'].isin(valid_stay_ids)]\n",
    "\n",
    "if include_cxr:\n",
    "    cxr_df = cxr_df[cxr_df['stay_id'].isin(valid_stay_ids)]\n",
    "\n",
    "if include_ecg:\n",
    "    ecg_df = ecg_df[ecg_df['stay_id'].isin(valid_stay_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_df = pd.read_csv(os.path.join(mimic_iv_path, \"core\", \"admissions.csv.gz\"))\n",
    "admissions_df = admissions_df.rename(columns={\"hospital_expire_flag\": \"died\"})\n",
    "admissions_df = admissions_df[[\"subject_id\", \"hadm_id\", \"died\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ireg_vitals_ts_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of stays with ecg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_stays)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     unique_stays \u001b[38;5;241m=\u001b[39m \u001b[43mireg_vitals_ts_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of stays with vitals: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_stays)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_notes:\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# Get stays with either TS or notes\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ireg_vitals_ts_df' is not defined"
     ]
    }
   ],
   "source": [
    "if not include_missing:\n",
    "    unique_stays = ireg_vitals_ts_df['stay_id'].unique()\n",
    "    print(f\"Number of stays with vitals: {len(unique_stays)}\")\n",
    "\n",
    "    if include_notes:\n",
    "        unique_stays = np.intersect1d(unique_stays, notes_df['stay_id'].unique())\n",
    "        print(f\"Number of stays with notes: {len(unique_stays)}\")\n",
    "\n",
    "    if include_cxr:\n",
    "        unique_stays = np.intersect1d(unique_stays, cxr_df['stay_id'].unique())\n",
    "        print(f\"Number of stays with cxr: {len(unique_stays)}\")\n",
    "\n",
    "    if include_ecg:\n",
    "        unique_stays = np.intersect1d(unique_stays, ecg_df['stay_id'].unique())\n",
    "        print(f\"Number of stays with ecg: {len(unique_stays)}\")        \n",
    "else:\n",
    "    unique_stays = ireg_vitals_ts_df['stay_id'].unique()\n",
    "    print(f\"Number of stays with vitals: {len(unique_stays)}\")\n",
    "\n",
    "    if include_notes:\n",
    "        # Get stays with either TS or notes\n",
    "        unique_stays = np.union1d(unique_stays, notes_df['stay_id'].unique())\n",
    "        print(f\"Number of stays with either TS or notes: {len(unique_stays)}\")\n",
    "    \n",
    "    if include_cxr:\n",
    "        unique_stays = np.union1d(unique_stays, cxr_df['stay_id'].unique())\n",
    "        print(f\"Number of stays with either TS, notes, cxr: {len(unique_stays)}\")\n",
    "\n",
    "    if include_ecg:\n",
    "        unique_stays = np.union1d(unique_stays, ecg_df['stay_id'].unique())\n",
    "        print(f\"Number of stays with either TS, notes, cxr, ecg: {len(unique_stays)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, test splits\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(unique_stays)\n",
    "train_stays = unique_stays[:int(0.7*len(unique_stays))]\n",
    "val_stays = unique_stays[int(0.7*len(unique_stays)):int(0.85*len(unique_stays))]\n",
    "test_stays = unique_stays[int(0.85*len(unique_stays)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ireg_ts_df = ireg_vitals_ts_df[ireg_vitals_ts_df['stay_id'].isin(train_stays)].copy()\n",
    "train_imputed_df = imputed_vitals[imputed_vitals['stay_id'].isin(train_stays)].copy()\n",
    "\n",
    "cols = train_ireg_ts_df.columns.tolist()\n",
    "cols = [col for col in cols if col not in ['subject_id', 'hadm_id', 'stay_id', 'timedelta']]\n",
    "\n",
    "if standard_scale:\n",
    "    for col in cols:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_ireg_ts_df[[col]])\n",
    "        ireg_vitals_ts_df[col] = scaler.transform(ireg_vitals_ts_df[[col]])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_imputed_df[[col]])\n",
    "        imputed_vitals[col] = scaler.transform(imputed_vitals[[col]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2194/24591 [00:24<04:07, 90.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24591/24591 [04:27<00:00, 92.09it/s]\n",
      "100%|██████████| 5270/5270 [00:57<00:00, 92.00it/s]\n",
      " 32%|███▏      | 1686/5270 [00:18<00:38, 94.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5270/5270 [00:57<00:00, 92.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_stay_list(stays):\n",
    "    stays_list = []\n",
    "\n",
    "    for curr_stay in tqdm(stays):\n",
    "        curr_stay_ireg = ireg_vitals_ts_df[ireg_vitals_ts_df['stay_id'] == curr_stay].copy()\n",
    "        curr_stay_imputed = imputed_vitals[imputed_vitals['stay_id'] == curr_stay].copy()\n",
    "        curr_icustay = icustays_df[icustays_df['stay_id'] == curr_stay].copy()\n",
    "\n",
    "        try:\n",
    "            curr_hadm_id = curr_stay_ireg['hadm_id'].iloc[0]\n",
    "            died = admissions_df[admissions_df['hadm_id'] == curr_hadm_id]['died'].iloc[0]\n",
    "        except:\n",
    "            print(\"error!\")\n",
    "            continue\n",
    "\n",
    "        intime = icustays_df[icustays_df['stay_id'] == curr_stay]['intime'].iloc[0]\n",
    "        outtime = icustays_df[icustays_df['stay_id'] == curr_stay]['outtime'].iloc[0]\n",
    "        icu_time_delta = (outtime - intime).total_seconds() / 3600\n",
    "\n",
    "        if died == 1:\n",
    "            pass\n",
    "\n",
    "        if (icu_time_delta < 96) & (died == 0):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        if len(curr_stay_ireg) == 0:\n",
    "            continue\n",
    "\n",
    "        if include_notes:\n",
    "            curr_stay_notes = notes_df[notes_df['stay_id'] == curr_stay].copy()\n",
    "\n",
    "        if include_cxr:\n",
    "            curr_stay_cxr = cxr_df[cxr_df['stay_id'] == curr_stay].copy()\n",
    "\n",
    "        curr_stay_dict = {}\n",
    "        curr_stay_dict['name'] = curr_stay_ireg['subject_id'].iloc[0]\n",
    "        curr_stay_dict['hadm_id'] = curr_stay_ireg['hadm_id'].iloc[0]\n",
    "        curr_stay_dict['stay_id'] = curr_stay\n",
    "        curr_stay_dict['ts_tt'] = curr_stay_ireg['timedelta'].values\n",
    "\n",
    "        curr_stay_ireg.drop(columns=['subject_id', 'hadm_id', 'stay_id', 'timedelta'], inplace=True)\n",
    "        ireg_ts_mask = curr_stay_ireg.notnull()\n",
    "        curr_stay_ireg.fillna(0, inplace=True)\n",
    "        curr_stay_dict['irg_ts'] = curr_stay_ireg.values\n",
    "        curr_stay_dict['irg_ts_mask'] = ireg_ts_mask.values.astype(int)\n",
    "\n",
    "        curr_stay_imputed.drop(columns=['subject_id', 'hadm_id', 'stay_id', 'timedelta'], inplace=True)\n",
    "        curr_stay_dict['reg_ts'] = curr_stay_imputed.values\n",
    "\n",
    "        if include_notes:\n",
    "            if len(curr_stay_notes) == 0:\n",
    "                curr_stay_dict['text_data'] = []\n",
    "                curr_stay_dict['text_time_to_end'] = []\n",
    "                curr_stay_dict['text_embeddings'] = []\n",
    "                curr_stay_dict['text_missing'] = 1\n",
    "            else:\n",
    "                curr_stay_dict['text_data'] = curr_stay_notes['text'].tolist()\n",
    "                curr_stay_dict['text_time_to_end'] = curr_stay_notes['icu_time_delta'].values\n",
    "                curr_stay_dict['text_embeddings'] = [emb[0][0] for emb in curr_stay_notes['biobert_embeddings']]\n",
    "                curr_stay_dict['text_missing'] = 0\n",
    "\n",
    "        if include_cxr:\n",
    "            if len(curr_stay_cxr) == 0:\n",
    "                curr_stay_dict['cxr_feats'] = []\n",
    "                curr_stay_dict['cxr_time'] = []\n",
    "                curr_stay_dict['cxr_missing'] = 1\n",
    "            else:\n",
    "                curr_stay_dict['cxr_feats'] = curr_stay_cxr['densefeatures'].tolist()\n",
    "                curr_stay_dict['cxr_time'] = curr_stay_cxr['icu_time_delta'].values\n",
    "                curr_stay_dict['cxr_missing'] = 0\n",
    "\n",
    "        if include_ecg:\n",
    "            curr_stay_ecg = ecg_df[ecg_df['stay_id'] == curr_stay].copy()\n",
    "            if len(curr_stay_ecg) == 0:\n",
    "                curr_stay_dict['ecg_feats'] = []\n",
    "                curr_stay_dict['ecg_time'] = []\n",
    "                curr_stay_dict['ecg_missing'] = 1\n",
    "            else:\n",
    "                curr_stay_dict['ecg_feats'] = curr_stay_ecg['embeddings'].tolist()\n",
    "                curr_stay_dict['ecg_time'] = curr_stay_ecg['icu_time_delta'].values\n",
    "                curr_stay_dict['ecg_missing'] = 0\n",
    "\n",
    "        curr_stay_dict['label'] = label\n",
    "        stays_list.append(curr_stay_dict)\n",
    "\n",
    "    return stays_list\n",
    "\n",
    "train_stays_list = get_stay_list(train_stays)\n",
    "val_stays_list = get_stay_list(val_stays)\n",
    "test_stays_list = get_stay_list(test_stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train stays to /cis/home/charr165/Documents/multimodal/preprocessing/train_los-48-cxr-notes-ecg-missingInd_stays.pkl\n",
      "Saving val stays to /cis/home/charr165/Documents/multimodal/preprocessing/val_los-48-cxr-notes-ecg-missingInd_stays.pkl\n",
      "Saving test stays to /cis/home/charr165/Documents/multimodal/preprocessing/test_los-48-cxr-notes-ecg-missingInd_stays.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the data\n",
    "import pickle\n",
    "\n",
    "restrict_48_hours = True\n",
    "\n",
    "base_name = \"los\"\n",
    "if restrict_48_hours:\n",
    "    base_name += \"-48\"\n",
    "else:\n",
    "    base_name += \"-all\"\n",
    "\n",
    "if include_cxr:\n",
    "    if include_notes:\n",
    "        base_name += \"-cxr-notes\"\n",
    "    else:\n",
    "        base_name += \"-cxr\"\n",
    "\n",
    "if include_ecg:\n",
    "    base_name += \"-ecg\"\n",
    "\n",
    "if include_missing:\n",
    "    base_name += \"-missingInd\"\n",
    "\n",
    "f_path = os.path.join(output_dir, f\"train_{base_name}_stays.pkl\")\n",
    "with open(f_path, 'wb') as f:\n",
    "    print(f\"Saving train stays to {f_path}\")\n",
    "    pickle.dump(train_stays_list, f)\n",
    "\n",
    "f_path = os.path.join(output_dir, f\"val_{base_name}_stays.pkl\")\n",
    "with open(f_path, 'wb') as f:\n",
    "    print(f\"Saving val stays to {f_path}\")\n",
    "    pickle.dump(val_stays_list, f)\n",
    "\n",
    "f_path = os.path.join(output_dir, f\"test_{base_name}_stays.pkl\")\n",
    "with open(f_path, 'wb') as f:\n",
    "    print(f\"Saving test stays to {f_path}\")\n",
    "    pickle.dump(test_stays_list, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MulEHR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
